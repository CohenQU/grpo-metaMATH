# Config for 1 node of 8 x H100s

# Model arguments
model_name_or_path: hf-cmu-collab/Llama-3.2-3B-Instruct_backtrack_suffix_iteration1
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2
bf16: true
tf32: true
output_dir: data/Llama-3.1-8B-Instruct-grpo-v04.01
 
# Dataset arguments
dataset_name: hf-cmu-collab/metaMATH-Llama-3.1-8B-Instruct-GRPO
dataset_num_proc: 12
dataset_start: 0
dataset_end: 20_000

# Script args
alpha: 1.6
final_reward_weight: zero
reward_funcs:
  - final
  - info_gain
 
# Training arguments
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 32
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 1.0e-6
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.01
max_prompt_length: 1500
max_completion_length: 1024
num_generations: 4
use_vllm: true
vllm_gpu_memory_utilization: 0.8
temperature: 0.9
 
# Logging arguments
logging_strategy: steps
logging_steps: 1
report_to:
- wandb
eval_strategy: 'no'
save_strategy: steps
save_steps: 10
save_total_limit: 1
seed: 42
 
# Hugging Face Hub 
push_to_hub: true
hub_model_id: hf-cmu-collab/Llama-3.1-8B-Instruct-grpo-v04.01
hub_private_repo: true
hub_strategy: every_save