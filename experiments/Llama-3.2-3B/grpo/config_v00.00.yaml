# Config for 1 node of 8 x H100s

# Model arguments
model_name_or_path: hf-cmu-collab/Llama-3.1-8B-Instruct-sft_suffix-v12.00
model_revision: 612831f91e79314fcd38f1a189a3078d7900e214
torch_dtype: bfloat16
attn_implementation: flash_attention_2
bf16: true
tf32: true
output_dir: data/Llama-3.1-8B-Instruct-sft_suffix-v27.00
 
# Dataset arguments
dataset_name: hf-cmu-collab/metaMATH_20000_40000_v12.00_suffix_highest_pass_1_reward
dataset_num_proc: 12
max_seq_length: 2048
 
# Training arguments
num_train_epochs: 3
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 1.0e-6
lr_scheduler_type: linear
warmup_ratio: 0.0
 
# Logging arguments
logging_strategy: steps
logging_steps: 5
report_to:
- wandb
eval_strategy: steps
eval_steps: 100
save_strategy: steps
save_steps: 25
save_total_limit: 1
seed: 42
 
# Hugging Face Hub 
push_to_hub: true
hub_model_id: hf-cmu-collab/Llama-3.1-8B-Instruct-sft_suffix-v27.00
hub_private_repo: true
hub_strategy: every_save